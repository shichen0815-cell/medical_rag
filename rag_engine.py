# rag_engine.py
import os
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com" #ä½¿ç”¨å›½å†…é•œåƒ

import logging
from typing import List, Optional
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import HuggingFacePipeline
from langchain_qdrant import Qdrant
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

# ç¦ç”¨è­¦å‘Š
logging.getLogger("langchain").setLevel(logging.ERROR)
os.environ["TRANSFORMERS_NO_ADVISORY_WARNINGS"] = "1"


class MedicalRAG:
    def __init__(self, data_path: str = "data/", collection_name: str = "medical_db"):
        self.data_path = data_path
        # === æ–°å¢žï¼šè‡ªåŠ¨å°† PDF è½¬ä¸º TXT ===
        self._auto_convert_pdfs()
        self.collection_name = collection_name
        self._init_models()
        try:
            self._load_and_index_documents()
        except Exception as e:
            print(f"å‘é‡åº“æž„å»ºå¤±è´¥: {e}")
            raise
        self._build_rag_chain()

    def _auto_convert_pdfs(self):
        """
        æ‰«æ data_path ç›®å½•ï¼Œå°†æ‰€æœ‰ .pdf æ–‡ä»¶è½¬æ¢ä¸º .txtï¼ˆå¦‚æžœå°šæœªè½¬æ¢ï¼‰ã€‚
        è½¬æ¢åŽçš„æ–‡ä»¶ä¿å­˜åœ¨åŒä¸€ç›®å½•ï¼Œæ–‡ä»¶åä¿æŒä¸€è‡´ï¼ˆä»…æ‰©å±•åå˜åŒ–ï¼‰ã€‚
        """
        try:
            import os
            from pathlib import Path

            data_dir = Path(self.data_path)
            if not data_dir.exists():
                data_dir.mkdir(parents=True, exist_ok=True)
                return

            pdf_files = list(data_dir.glob("*.pdf"))
            if not pdf_files:
                return  # æ—  PDFï¼Œè·³è¿‡

            # æ£€æŸ¥å“ªäº› PDF å°šæœªè½¬æ¢ï¼ˆå³æ²¡æœ‰åŒå .txtï¼‰
            to_convert = []
            for pdf in pdf_files:
                txt_path = pdf.with_suffix(".txt")
                if not txt_path.exists():
                    to_convert.append(pdf)

            if not to_convert:
                return  # æ‰€æœ‰ PDF å·²è½¬æ¢

            print(f"ðŸ” æ£€æµ‹åˆ° {len(to_convert)} ä¸ªæœªè½¬æ¢çš„ PDF æ–‡ä»¶ï¼Œæ­£åœ¨è‡ªåŠ¨è½¬æ¢ä¸º TXT...")

            # åŠ¨æ€å¯¼å…¥è½¬æ¢å‡½æ•°
            from pdf_to_txt_clean import convert_pdf_to_txt

            for pdf_path in to_convert:
                print(f"  â†’ è½¬æ¢: {pdf_path.name}")
                try:
                    convert_pdf_to_txt(str(pdf_path), output_dir=str(data_dir))
                except Exception as e:
                    print(f"è½¬æ¢å¤±è´¥ {pdf_path.name}: {e}")

            print("PDF è½¬ TXT å®Œæˆã€‚\n")

        except Exception as e:
            print(f"PDF è‡ªåŠ¨è½¬æ¢è¿‡ç¨‹å‡ºé”™: {e}")

    def _init_models(self):
        print("åŠ è½½åµŒå…¥æ¨¡åž‹ (BAAI/bge-m3)...")
        try:
            # å°è¯•åŠ è½½æœ¬åœ°æ¨¡åž‹
            model_path = "models/bge-m3"
            if os.path.exists(model_path):
                print(f"â†’ ä½¿ç”¨æœ¬åœ°æ¨¡åž‹: {model_path}")
                self.embedding_model = HuggingFaceEmbeddings(
                    model_name=model_path,
                    model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"},
                    encode_kwargs={"normalize_embeddings": True}
                )
            else:
                # ç½‘ç»œä¸‹è½½ï¼ˆå¤‡ç”¨ï¼‰
                self.embedding_model = HuggingFaceEmbeddings(
                    model_name="BAAI/bge-m3",
                    model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"},
                    encode_kwargs={"normalize_embeddings": True}
                )
        except Exception as e:
            print(f"åµŒå…¥æ¨¡åž‹åŠ è½½å¤±è´¥: {str(e)}")
            print("â†’ ä½¿ç”¨è½»é‡çº§å¤‡ç”¨æ¨¡åž‹: sentence-transformers/all-MiniLM-L6-v2")
            self.embedding_model = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={"device": "cuda" if torch.cuda.is_available() else "cpu"}
            )

        print("åŠ è½½é‡æŽ’å™¨ (BAAI/bge-reranker-v2-m3)...")
        try:
            reranker_path = "models/bge-reranker-v2-m3"
            if os.path.exists(reranker_path):
                print(f"â†’ ä½¿ç”¨æœ¬åœ°é‡æŽ’å™¨æ¨¡åž‹: {reranker_path}")
                self.reranker = HuggingFaceCrossEncoder(model_name=reranker_path)
            else:
                self.reranker = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-v2-m3")
        except Exception as e:
            print(f" é‡æŽ’å™¨åŠ è½½å¤±è´¥: {str(e)}")
            print("â†’ ä½¿ç”¨å¤‡ç”¨é‡æŽ’å™¨: cross-encoder/ms-marco-MiniLM-L-6-v2")
            self.reranker = HuggingFaceCrossEncoder(model_name="cross-encoder/ms-marco-MiniLM-L-6-v2")

        print("åŠ è½½æœ¬åœ° LLM (Qwen/Qwen2.5-0.5B-Instruct)...")
        try:
            llm_path = "models/qwen2.5-0.5b"
            if os.path.exists(llm_path):
                print(f"â†’ ä½¿ç”¨æœ¬åœ°LLM: {llm_path}")
                model = AutoModelForCausalLM.from_pretrained(
                    llm_path,
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    device_map="auto",
                    trust_remote_code=True
                )
                tokenizer = AutoTokenizer.from_pretrained(llm_path, trust_remote_code=True)
            else:
                model_name = "Qwen/Qwen2.5-0.5B-Instruct"
                print(f"â†’ ä»ŽHugging Faceä¸‹è½½LLM: {model_name} (è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ)")
                tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
                model = AutoModelForCausalLM.from_pretrained(
                    model_name,
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    device_map="auto",
                    trust_remote_code=True
                )

            pipe = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                max_new_tokens=256,
                temperature=0.3,
                do_sample=False,
                repetition_penalty=1.1
            )
            self.llm = HuggingFacePipeline(pipeline=pipe)
        except Exception as e:
            print(f"LLMåŠ è½½å¤±è´¥: {str(e)}")
            print("â†’ ä½¿ç”¨è¶…è½»é‡çº§æ¨¡åž‹: microsoft/Phi-3-mini-4k-instruct (éœ€è¦åœ¨çº¿ä¸‹è½½)")
            try:
                from transformers import Phi3ForCausalLM, AutoTokenizer
                model = Phi3ForCausalLM.from_pretrained(
                    "microsoft/Phi-3-mini-4k-instruct",
                    device_map="auto",
                    torch_dtype="auto",
                    trust_remote_code=True,
                )
                tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")
                pipe = pipeline(
                    "text-generation",
                    model=model,
                    tokenizer=tokenizer,
                    max_new_tokens=256,
                    temperature=0.3
                )
                self.llm = HuggingFacePipeline(pipeline=pipe)
            except Exception as e2:
                print(f" å¤‡ç”¨LLMåŠ è½½å¤±è´¥: {str(e2)}")
                print(" æ— æ³•åŠ è½½ä»»ä½•è¯­è¨€æ¨¡åž‹ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿žæŽ¥æˆ–åˆ›å»º models/ ç›®å½•å¹¶æ”¾ç½®æœ¬åœ°æ¨¡åž‹ã€‚")
                raise

    def _load_and_index_documents(self):
        print("åŠ è½½åŒ»å­¦æ–‡æ¡£...")
        documents = []
        for file in os.listdir(self.data_path):
            if file.endswith(".txt"):
                loader = TextLoader(os.path.join(self.data_path, file), encoding="utf-8")
                docs = loader.load()
                for doc in docs:
                    doc.metadata["source"] = file
                documents.extend(docs)

        if not documents:
            raise ValueError("æœªæ‰¾åˆ°ä»»ä½•åŒ»å­¦æ–‡æ¡£ï¼è¯·æ£€æŸ¥ data/ ç›®å½•ã€‚")

        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=400,
            chunk_overlap=50,
            separators=["\n\n", "\n", "ã€‚", "ï¼›", " "]
        )
        chunks = text_splitter.split_documents(documents)

        print(f"å…±ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—ï¼Œæ­£åœ¨æž„å»ºå‘é‡åº“...")
        # self.vector_store = Qdrant.from_documents(
        #     chunks,
        #     self.embedding_model,
        #     location=":memory:",
        #     collection_name=self.collection_name
        # )
        #  Qdrant ï¼Œæ”¹ç”¨ç£ç›˜æŒä¹…åŒ–
        self.vector_store = Qdrant.from_documents(
            chunks,
            self.embedding_model,
            path="./qdrant_db",  # æŒä¹…åŒ–åˆ°æœ¬åœ°ç›®å½•
            collection_name=self.collection_name,
            force_recreate=True  # é¿å…é‡å¤è¦†ç›–ï¼ˆå¯é€‰ï¼‰
        )

        base_retriever = self.vector_store.as_retriever(search_kwargs={"k": 5})
        compressor = CrossEncoderReranker(model=self.reranker, top_n=3)
        self.retriever = ContextualCompressionRetriever(
            base_compressor=compressor,
            base_retriever=base_retriever
        )

    def _rewrite_query(self, query: str) -> str:
        rewrite_rules = {
            "è¡€ç³–é«˜": "é«˜è¡€ç³–",
            "ç³–å°¿": "ç³–å°¿ç—…",
            "æ€€å­•è¡€ç³–": "å¦Šå¨ æœŸç³–å°¿ç—…"
        }
        rewritten = query
        for k, v in rewrite_rules.items():
            rewritten = rewritten.replace(k, v)
        if rewritten != query:
            print(f"åŽŸå§‹æŸ¥è¯¢: {query} â†’ é‡å†™åŽ: {rewritten}")
        return rewritten

    def _build_rag_chain(self):
        def format_docs_with_sources(docs):
            if not docs or all(not doc.page_content.strip() for doc in docs):
                return {"context": "", "sources": ""}
            context = "\n\n".join(doc.page_content for doc in docs)
            sources = sorted(set(doc.metadata.get("source", "æœªçŸ¥æ¥æº") for doc in docs))
            return {"context": context, "sources": "ã€".join(sources)}

        prompt_template = """
    ä½ æ˜¯ä¸€åä¸“ä¸šåŒ»ç–—åŠ©æ‰‹ï¼Œè¯·ä¸¥æ ¼ä¾æ®ä»¥ä¸‹åŒ»å­¦èµ„æ–™å›žç­”é—®é¢˜ã€‚
    å¦‚æžœèµ„æ–™ä¸­æ— ç›¸å…³ä¿¡æ¯ï¼Œè¯·å›žç­”ï¼š"æ ¹æ®å½“å‰çŸ¥è¯†åº“æ— æ³•å›žç­”è¯¥é—®é¢˜"ã€‚

    ã€åŒ»å­¦èµ„æ–™ã€‘
    {context}

    ã€é—®é¢˜ã€‘
    {question}

    ã€è¦æ±‚ã€‘
    1. å›žç­”å¿…é¡»å‡†ç¡®ã€ç®€æ´ï¼›
    2. ç¦æ­¢ä»»ä½•çŒœæµ‹ã€ç¼–é€ æˆ–è¶…å‡ºèµ„æ–™èŒƒå›´çš„å»ºè®®ã€‚
    """
        prompt = ChatPromptTemplate.from_template(prompt_template)

        # 1. æ‰“å°åŽŸå§‹æŸ¥è¯¢ & é‡å†™åŽæŸ¥è¯¢
        def log_and_rewrite(question_str):
            print(f"åŽŸå§‹æŸ¥è¯¢: {question_str}")
            rewritten = self._rewrite_query(question_str)
            print(f"é‡å†™åŽæŸ¥è¯¢: {rewritten}")
            return {"rewritten_q": rewritten, "original_q": question_str}

        input_mapper = RunnableLambda(log_and_rewrite)

        # 2. æ‰“å°é‡æŽ’åŽçš„æ–‡æœ¬ï¼ˆå³ retriever è¾“å‡ºï¼‰
        def retrieve_and_log(x):
            docs = self.retriever.invoke(x["rewritten_q"])
            print("\né‡æŽ’åŽè¿”å›žçš„æ–‡æœ¬å—ï¼ˆTop 3ï¼‰:")
            for i, doc in enumerate(docs, 1):
                content = doc.page_content.strip()[:200].replace('\n', ' ')
                source = doc.metadata.get("source", "æœªçŸ¥")
                print(f"  [{i}] æ¥æº: {source} | å†…å®¹: {content}...")
            print()
            return {"docs": docs, "question": x["original_q"]}

        # 3. æ‰“å°æœ€ç»ˆæ³¨å…¥æç¤ºè¯çš„ä¸Šä¸‹æ–‡
        def prepare_prompt_input(x):
            info = format_docs_with_sources(x["docs"])
            context = info["context"]
            sources = info["sources"]
            question = x["question"]
            print("æœ€ç»ˆæç¤ºè¯ï¼ˆPromptï¼‰è¾“å…¥å†…å®¹:")
            final_prompt = prompt.format(context=context, question=question)
            print(final_prompt)
            print("\n" + "=" * 60 + "\n")
            return {
                "context": context,
                "sources": sources,
                "question": question
            }

        base_chain = (
                input_mapper
                | RunnableLambda(retrieve_and_log)
                | RunnableLambda(prepare_prompt_input)
                | {
                    "answer": prompt | self.llm | StrOutputParser(),
                    "sources": lambda x: x["sources"],
                }
        )

        def finalize_output(inputs):
            answer = inputs["answer"]
            sources = inputs["sources"]
            if not sources or "æ ¹æ®å½“å‰çŸ¥è¯†åº“æ— æ³•å›žç­”" in answer:
                return answer
            return f"{answer}\n\nï¼ˆèµ„æ–™æ¥æºï¼š{sources}ï¼‰"

        self.rag_chain = base_chain | RunnableLambda(finalize_output)
    def ask(self, question: str) -> str:
        return self.rag_chain.invoke(question)  # ç›´æŽ¥ä¼ å­—ç¬¦ä¸²